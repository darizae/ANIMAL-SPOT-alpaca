#!/usr/bin/env python3
"""
RF post-processing for ANIMAL-SPOT benchmark runs.

Given a run_root (…/BENCHMARK/runs/<model>/<variant>), this script:
  • reads the evaluation selection tables (…/evaluation/annotations/*.txt)
  • extracts Python features for each selection (robust spectral + MFCC)
  • passes through the CNN logit (mean) column if present
  • loads a pickled RandomForestClassifier
  • scores each selection with rf_prob (P(target)) and keeps rf_prob >= threshold
  • writes RF-filtered selection tables to …/postrf/annotations/
  • writes …/postrf/index.json, mirroring tools/build_pred_index.py schema + rf meta

It can be run locally or via Slurm arrays generated by tools/rf_factory.py.
"""

from __future__ import annotations

import argparse
import json
import re
import sys
from datetime import datetime
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
import soundfile as sf

# Reuse your in-repo feature code
sys.path.append(str(Path(__file__).resolve().parents[0]))  # random_forest/
from audio_features import raven_robust_features, mfcc_summary

# ───────────────────────────── helpers ──────────────────────────────

SEL_HEADER = [
    "Selection", "View", "Channel",
    "Begin time (s)", "End time (s)", "Low Freq (Hz)", "High Freq (Hz)",
    "CNN logit (mean)", "Sound type", "Comments"
]

_WAV_FROM_TABLE = re.compile(
    r"^(?P<stem>.+?)_predict_output(?:\.log)?\.annotation\.result\.txt$", re.IGNORECASE
)


def read_sel_table(path: Path) -> pd.DataFrame:
    df = pd.read_csv(path, sep="\t")
    # tolerate older header without CNN column
    if "CNN logit (mean)" not in df.columns:
        df["CNN logit (mean)"] = np.nan
    # canonical columns (we keep potential extra columns but won’t use them)
    needed = {"Selection", "Begin time (s)", "End time (s)", "Low Freq (Hz)", "High Freq (Hz)"}
    missing = needed - set(df.columns)
    if missing:
        raise ValueError(f"{path.name} missing columns: {missing}")
    return df


def derive_wave_from_table_name(txt_path: Path) -> str:
    m = _WAV_FROM_TABLE.match(txt_path.name)
    if m:
        return f"{m.group('stem')}.wav"
    else:
        raise Exception(f"Error matching selection table name to its corresponding WAV file. Failing path: {txt_path}")


def load_wave(audio_root: Path, wave_name: str) -> tuple[np.ndarray, int]:
    wav = audio_root / wave_name
    if not wav.exists() and wav.with_suffix(".WAV").exists():
        wav = wav.with_suffix(".WAV")
    y, sr = sf.read(wav, always_2d=False)
    if y.ndim > 1: y = y.mean(axis=1)
    return y.astype(np.float32, copy=False), sr


def sigmoid(x):  # only used if you want prob from a logit column
    return 1.0 / (1.0 + np.exp(-x))


# ───────────────────────────── core ─────────────────────────────────

def process_run(
        run_root: Path,
        audio_root: Path,
        rf_model_path: Path,
        features_choice: str,  # "spectral" | "mfcc" | "union"
        rf_threshold: float,
        n_fft: int,
        hop: int,
        n_mfcc: int,
        include_deltas: bool = True,
):
    eval_ann_dir = run_root / "evaluation" / "annotations"
    out_root = run_root / "postrf"
    out_ann_dir = out_root / "annotations"
    out_feat_dir = out_root / "features_py"
    out_ann_dir.mkdir(parents=True, exist_ok=True)
    out_feat_dir.mkdir(parents=True, exist_ok=True)

    # collect all selection tables
    sel_paths = sorted(eval_ann_dir.glob("*.annotation.result.txt"))
    if not sel_paths:
        raise SystemExit(f"No selection tables found in {eval_ann_dir}")

    # lazy-load RF
    rf = joblib.load(rf_model_path)

    # build per-table outputs and an index
    index_entries = []
    uid = 1

    for sel_path in sel_paths:
        wave_file = derive_wave_from_table_name(sel_path)
        df = read_sel_table(sel_path)
        # cache audio
        y, sr = load_wave(audio_root, wave_file)

        # extract feature rows
        rows = []
        kept_rows = []
        for _, r in df.iterrows():
            start = float(r["Begin time (s)"]);
            end = float(r["End time (s)"])
            fmin = float(r.get("Low Freq (Hz)", 0.0));
            fmax = float(r.get("High Freq (Hz)", 0.0)) or None

            spec = raven_robust_features(y, sr, start, end, fmin=fmin, fmax=fmax, n_fft=n_fft, hop_length=hop)
            mfc = mfcc_summary(y, sr, start, end, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop,
                               include_deltas=include_deltas)
            base = {"wave_file": wave_file, "selection": int(r["Selection"])}

            # cnn logit passthrough if present
            if "CNN logit (mean)" in df.columns and not pd.isna(r["CNN logit (mean)"]):
                base["cnn_logit_mean"] = float(r["CNN logit (mean)"])

            # stack features according to choice
            if features_choice == "spectral":
                feat = {**spec}
            elif features_choice == "mfcc":
                feat = {**mfc}
            else:
                feat = {**spec, **mfc}
            X_row = {**base, **feat}
            rows.append(X_row)

        feat_df = pd.DataFrame(rows)
        # keep the feature file (optional but nice for debugging)
        feat_out = out_feat_dir / f"{sel_path.stem}.{features_choice}.csv"
        feat_df.to_csv(feat_out, index=False)

        # vectorise: drop ID cols
        X = feat_df.drop(columns=[c for c in ["wave_file", "selection"] if c in feat_df.columns])
        # model may expect cnn_logit_mean; if absent, fill with 0
        if "cnn_logit_mean" in rf.feature_names_in_ and "cnn_logit_mean" not in X.columns:
            X["cnn_logit_mean"] = 0.0
        # align columns if model trained on a specific set/order
        if hasattr(rf, "feature_names_in_"):
            # add any missing columns with zeros; drop extras
            for col in rf.feature_names_in_:
                if col not in X.columns: X[col] = 0.0
            X = X[rf.feature_names_in_]

        rf_prob = rf.predict_proba(X)[:, 1]
        feat_df["rf_prob"] = rf_prob
        feat_df["rf_pred"] = (rf_prob >= rf_threshold).astype(int)

        # write a new selection table keeping only rf_pred == 1
        kept = df.copy()
        kept["RF prob"] = rf_prob
        kept["RF pred"] = (rf_prob >= rf_threshold).astype(int)
        kept = kept[kept["RF pred"] == 1].drop(columns=["RF pred"])
        # write with canonical header (keep CNN logit, RF prob in the extra slot)
        # we’ll map to the 10 columns in SEL_HEADER; RF prob goes into Comments as "rf_prob=<val>".
        out_lines = ["\t".join(SEL_HEADER)]
        it = 1
        for i, r in kept.iterrows():
            cnn_col = ("" if pd.isna(r.get("CNN logit (mean)", np.nan)) else str(r["CNN logit (mean)"]))
            sound_type = r.get("Sound type", "target")
            comment = f"rf_prob={r['RF prob']:.6f}"
            out_lines.append(
                f"{it}\tSpectrogram_1\t1\t{r['Begin time (s)']}\t{r['End time (s)']}\t"
                f"{r.get('Low Freq (Hz)', 0)}\t{r.get('High Freq (Hz)', 0)}\t"
                f"{cnn_col}\t{sound_type}\t{comment}"
            )
            # index entry
            index_entries.append({
                "uid": uid,
                "pred_path": str((out_ann_dir / sel_path.name).relative_to(run_root)),
                "tape": wave_file,
                "start_s": float(r["Begin time (s)"]),
                "end_s": float(r["End time (s)"]),
                "dur_s": round(float(r["End time (s)"]) - float(r["Begin time (s)"]), 3),
                "rf_prob": float(r["RF prob"]),
            })
            uid += 1
            it += 1

        out_sel = out_ann_dir / sel_path.name  # same base name
        out_sel.write_text("\n".join(out_lines) + "\n")

    # postrf index
    meta = {
        "generated_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
        "rf_model": str(rf_model_path),
        "features_choice": features_choice,
        "rf_threshold": rf_threshold,
        "n_entries": len(index_entries),
    }
    (run_root / "postrf" / "index.json").write_text(json.dumps({"meta": meta, "entries": index_entries}, indent=2))
    print(f"✓ RF filtered selections → {run_root / 'postrf/annotations'}")
    print(f"✓ post-RF index.json → {run_root / 'postrf/index.json'}")


# ───────────────────────────── CLI ──────────────────────────────────

def parse_kv_cfg(path: Path) -> dict[str, str]:
    cfg = {}
    for ln in path.read_text().splitlines():
        ln = ln.strip()
        if not ln or ln.startswith("#"): continue
        if "=" in ln:
            k, v = ln.split("=", 1)
            cfg[k.strip()] = v.strip()
    return cfg


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("cfg", help="Path to rf.cfg (key=value lines) OR a BENCHMARK run_root")
    args = ap.parse_args()
    p = Path(args.cfg).resolve()

    if p.is_file() and p.name.endswith(".cfg"):
        kv = parse_kv_cfg(p)
        run_root = Path(kv["run_root"]).resolve()
        audio_root = Path(kv["audio_dir"]).resolve()
        rf_model_path = Path(kv["rf_model_path"]).resolve()
        features_choice = kv.get("features_choice", "union")
        rf_threshold = float(kv.get("rf_threshold", "0.35"))
        n_fft = int(kv.get("n_fft", "2048"))
        hop = int(kv.get("hop", "512"))
        n_mfcc = int(kv.get("n_mfcc", "13"))
        include_deltas = kv.get("include_deltas", "true").lower() == "true"
    else:
        # assume BENCHMARK/runs/... passed directly (for local quick tests)
        run_root = p
        # sensible defaults for local tests
        # You can edit these literals as you wish
        audio_root = run_root.parents[2] / "data" / "benchmark_corpus_v1" / "labelled_recordings"
        rf_model_path = Path("random_forest/models/rf_features_with_labels_neg1.pkl").resolve()
        features_choice = "union"
        rf_threshold = 0.35
        n_fft, hop, n_mfcc, include_deltas = 2048, 512, 13, True

    process_run(run_root, audio_root, rf_model_path, features_choice, rf_threshold, n_fft, hop, n_mfcc, include_deltas)


if __name__ == "__main__":
    main()
